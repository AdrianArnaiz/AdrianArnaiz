---
title: "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs"
authors:
- admin
- Miguel Baidal
- Erik Derner
- Jenn Layton Annable
- Mark Ball
- Mark Ince
- Elvira Perez Vallejos
- Nuria Oliver

date: "2025-09-29T00:00:00Z"
doi: "10.48550/arXiv.2509.24857"

# Schedule page publish date (NOT publication's date).
publishDate: "2025-09-30T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: Preprint
publication_short: In *[arXiv:2509.24857](https://arxiv.org/abs/2509.24857)*

abstract: "The widespread use of chatbots powered by large language models (LLMs) has transformed how people seek advice across domains, including high-stakes contexts such as mental health support. Despite their scalability, LLMs' ability to safely detect and respond to acute mental health crises remains poorly understood due to the absence of unified taxonomies, annotated benchmarks, and clinically grounded evaluations. This work introduces a unified taxonomy of six crisis types, a curated evaluation dataset, and an expert-designed protocol for assessing response safety and appropriateness. We benchmark three state-of-the-art LLMs on their ability to classify crisis types and generate safe responses. While LLMs often provide consistent support for explicit crises, significant risks persist: a notable proportion of responses are harmful or inappropriate, especially from open-weight models. We also uncover systemic weaknesses in handling indirect signals, overreliance on formulaic replies, and misalignment with user context. Our framework lays the foundation for responsible innovation in AI-driven mental health support, aimed at minimizing harm and improving crisis intervention."

# Summary. An optional shortened abstract.
summary: This work evaluates how LLMs handle mental health crises, introducing a unified taxonomy, benchmark dataset, and expert-based evaluation protocol — revealing both support capabilities and significant safety risks.

tags:
- Large Language Models
- Trustworthy AI

featured: true

links:
url_pdf: 'https://arxiv.org/pdf/2509.24857.pdf'
url_code: 'https://github.com/ellisalicante/LLMs-Mental-Health-Crisis'
url_dataset: 'https://github.com/ellisalicante/LLMs-Mental-Health-Crisis'
url_poster: ''
url_project: ''
url_slides: ''
url_PapersWithCode: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: 'LLMs and Mental Health: Balancing Help and Harm'
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
# projects: []

# Slides (optional).
# slides: ""
---

This preprint introduces a unified framework to evaluate how large language models (LLMs) detect and respond to mental health crises, combining a novel taxonomy, a curated dataset, and expert-based assessments. The findings reveal that, while LLMs can offer support in many situations, they also pose significant safety risks — particularly when handling subtle crisis signals. The work highlights critical challenges and provides a foundation for safer, more effective AI-driven mental health interventions.
