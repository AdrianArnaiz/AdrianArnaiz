---
title: "[ADITECH] Algorithmic Fairness in High-Stakes AI Systems (Sp)"

event: ADITECH - Masterclass on Algorithmic and Fairness
event_short: ADITECH
event_url: https://www.aditech.com/es/evento/sesgos-discriminacion-y-justicia-algoritmica-en-la-toma-de-decisiones/

location: Pamplona, Spain

summary: Seminar on algorithmic bias, discrimination risks, and fairness-aware AI systems in high-stakes decision-making contexts.
abstract: This session examined how algorithmic systems can reproduce or amplify social biases in high-impact decision-making domains such as employment, finance, and public administration. The talk addressed technical sources of bias, fairness metrics and mitigation strategies, as well as the broader sociotechnical and regulatory context surrounding algorithmic decision-making. Special attention was given to transparency, accountability, and the implications of the EU AI Act for organizations deploying AI systems.

# Talk start and end times.
date: "2025-12-15T09:00:00Z"
#date_end: ""
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: "2025-12-15T17:00:00Z"

authors: []
tags: [trustworthy-ai, algorithmic-fairness, ai-regulation, eu-ai-act, high-stakes-ai]

# Is this a featured talk? (true/false)
featured: false

image:
  caption: 'ADITECH ‚Äì Justicia Algor√≠tmica'
  focal_point: Center

links:
- icon: globe
  icon_pack: fas
  name: Event page
  url: https://www.aditech.com/es/evento/sesgos-discriminacion-y-justicia-algoritmica-en-la-toma-de-decisiones/
- icon: newspaper
  icon_pack: fas
  name: News coverage
  url: https://www.aditech.com/es/la-justicia-algoritmica-y-los-sesgos-en-inteligencia-artificial-centran-una-formacion-organizada-por-aditech/
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---



I delivered a seminar co-organized by [ADITECH](https://www.aditech.com) and [NAIR](https://www.naircenter.com/), focused on the technical and societal challenges of algorithmic decision-making systems. The session explored how machine learning models can introduce or amplify discrimination, particularly in high-stakes contexts such as hiring, credit allocation, or access to public services.

The talk combined:

* ‚öñÔ∏è **Algorithmic fairness foundations**: definitions of fairness, impossibility results, and trade-offs.
* üßÆ **Sources of bias**: data imbalance, label bias, proxy variables, feedback loops, and distribution shift.
* üõ†Ô∏è **Mitigation strategies**: pre-, in-, and post-processing techniques.
* üèõÔ∏è **Regulatory and governance perspective**: transparency obligations, risk categorization, and compliance considerations under the EU AI Act.
* üë•** Case study** (Viog√©n system): analysis of the Spanish VioG√©n algorithm used for gender-based violence risk assessment, examining transparency, risk scoring methodology, potential bias amplification, and governance challenges in public-sector AI deployment.

The discussion emphasized that fairness in AI is not only a technical optimization problem but a **sociotechnical design challenge** requiring alignment between modeling choices, institutional constraints, and legal frameworks.

